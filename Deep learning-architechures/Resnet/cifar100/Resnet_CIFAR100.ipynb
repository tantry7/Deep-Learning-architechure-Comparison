{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet-CIFAR100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrBzaefmts2x"
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar100\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50 \n",
        "data_augmentation = True\n",
        "num_classes = 100\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmoX7j59uEWO",
        "outputId": "5befb8ad-069e-4982-b0c4-ce44ff267c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nU59ctSuNqH"
      },
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=100):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=100):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "CvV-lPxCPZ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyjB9icGthBJ",
        "outputId": "cb802cf8-bd29-4e4a-dfcf-454ea1b0d1a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#if version == 2:\n",
        "model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "#else:\n",
        "    #model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar100_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  0.001\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 16)   272         ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 64)   1088        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 32, 64)   0           ['conv2d_4[0][0]',               \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 16)   1040        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 32, 64)   0           ['add[0][0]',                    \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 64)  256         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 64)   4160        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 128)  8320        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 16, 16, 128)  0           ['conv2d_11[0][0]',              \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 128)  512        ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 128)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 64)  256         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 64)  256         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 16, 16, 128)  0           ['add_2[0][0]',                  \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['add_3[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 8, 8, 128)    16512       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 8, 8, 128)   512         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 8, 8, 128)   512         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 8, 8, 256)    33024       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 8, 8, 256)    0           ['conv2d_18[0][0]',              \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 8, 8, 256)   1024        ['add_4[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_15[0][0]']          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 8, 8, 128)   512         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 8, 8, 128)   512         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 8, 8, 256)    0           ['add_4[0][0]',                  \n",
            "                                                                  'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 8, 8, 256)   1024        ['add_5[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 1, 1, 256)   0           ['activation_18[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 256)          0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          25700       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 597,220\n",
            "Trainable params: 593,732\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n",
            "Learning rate:  0.001\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - ETA: 0s - loss: 4.0288 - accuracy: 0.1368WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 55s 27ms/step - loss: 4.0288 - accuracy: 0.1368 - val_loss: 4.0998 - val_accuracy: 0.1495 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.3145 - accuracy: 0.2442WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 3.3145 - accuracy: 0.2442 - val_loss: 3.3009 - val_accuracy: 0.2454 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 3/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 2.9638 - accuracy: 0.3125WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.9641 - accuracy: 0.3125 - val_loss: 2.8786 - val_accuracy: 0.3292 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 4/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 2.7250 - accuracy: 0.3620WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.7252 - accuracy: 0.3620 - val_loss: 2.8398 - val_accuracy: 0.3469 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 5/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 2.5480 - accuracy: 0.4016WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.5478 - accuracy: 0.4017 - val_loss: 2.7736 - val_accuracy: 0.3785 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 6/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 2.4135 - accuracy: 0.4322WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.4132 - accuracy: 0.4323 - val_loss: 2.6368 - val_accuracy: 0.4020 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 2.3161 - accuracy: 0.4553WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 2.3161 - accuracy: 0.4553 - val_loss: 2.6663 - val_accuracy: 0.4039 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 8/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 2.2306 - accuracy: 0.4819WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.2305 - accuracy: 0.4819 - val_loss: 2.2635 - val_accuracy: 0.4830 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 9/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 2.1519 - accuracy: 0.5005WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.1517 - accuracy: 0.5006 - val_loss: 2.4539 - val_accuracy: 0.4422 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 10/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 2.0945 - accuracy: 0.5166WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0951 - accuracy: 0.5165 - val_loss: 2.2671 - val_accuracy: 0.4809 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 2.0472 - accuracy: 0.5253WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 2.0472 - accuracy: 0.5253 - val_loss: 2.3314 - val_accuracy: 0.4823 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 12/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 2.0000 - accuracy: 0.5383WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.9999 - accuracy: 0.5383 - val_loss: 2.2457 - val_accuracy: 0.5026 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 13/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.9558 - accuracy: 0.5525WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.9558 - accuracy: 0.5525 - val_loss: 2.1850 - val_accuracy: 0.5186 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 14/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.9190 - accuracy: 0.5644WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.9191 - accuracy: 0.5644 - val_loss: 2.1565 - val_accuracy: 0.5111 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.8869 - accuracy: 0.5741WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.8869 - accuracy: 0.5741 - val_loss: 2.6042 - val_accuracy: 0.4401 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 16/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.8518 - accuracy: 0.5837WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.8514 - accuracy: 0.5837 - val_loss: 2.2428 - val_accuracy: 0.5084 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 17/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.8278 - accuracy: 0.5901WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.8275 - accuracy: 0.5902 - val_loss: 2.1892 - val_accuracy: 0.5191 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.7938 - accuracy: 0.5967WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.7938 - accuracy: 0.5967 - val_loss: 2.1913 - val_accuracy: 0.5218 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 19/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.7793 - accuracy: 0.6044WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.7792 - accuracy: 0.6045 - val_loss: 2.2458 - val_accuracy: 0.5175 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 20/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.7491 - accuracy: 0.6120WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.7501 - accuracy: 0.6118 - val_loss: 2.0534 - val_accuracy: 0.5550 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 21/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.7281 - accuracy: 0.6171WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.7279 - accuracy: 0.6172 - val_loss: 2.2133 - val_accuracy: 0.5281 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 22/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.7156 - accuracy: 0.6212WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.7155 - accuracy: 0.6212 - val_loss: 2.4020 - val_accuracy: 0.5232 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.6969 - accuracy: 0.6267WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.6969 - accuracy: 0.6267 - val_loss: 2.4365 - val_accuracy: 0.5042 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 24/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.6805 - accuracy: 0.6323WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.6805 - accuracy: 0.6323 - val_loss: 2.0976 - val_accuracy: 0.5472 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 25/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.6653 - accuracy: 0.6365WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.6654 - accuracy: 0.6366 - val_loss: 2.0925 - val_accuracy: 0.5515 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 26/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.6521 - accuracy: 0.6401WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.6519 - accuracy: 0.6402 - val_loss: 2.2299 - val_accuracy: 0.5365 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 27/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.6310 - accuracy: 0.6479WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 1.6310 - accuracy: 0.6479 - val_loss: 2.0914 - val_accuracy: 0.5581 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 28/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.6189 - accuracy: 0.6498WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.6189 - accuracy: 0.6498 - val_loss: 1.9542 - val_accuracy: 0.5843 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 29/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.6058 - accuracy: 0.6559WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.6058 - accuracy: 0.6558 - val_loss: 1.9645 - val_accuracy: 0.5777 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.5919 - accuracy: 0.6584WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.5919 - accuracy: 0.6584 - val_loss: 2.0157 - val_accuracy: 0.5823 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 31/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.5831 - accuracy: 0.6621WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.5835 - accuracy: 0.6619 - val_loss: 2.2538 - val_accuracy: 0.5613 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 32/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.5763 - accuracy: 0.6628WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.5770 - accuracy: 0.6628 - val_loss: 2.0050 - val_accuracy: 0.5826 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 33/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.5647 - accuracy: 0.6665WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.5654 - accuracy: 0.6664 - val_loss: 2.2849 - val_accuracy: 0.5479 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.5554 - accuracy: 0.6712WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.5554 - accuracy: 0.6712 - val_loss: 2.1892 - val_accuracy: 0.5578 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 35/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.5505 - accuracy: 0.6704WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.5505 - accuracy: 0.6704 - val_loss: 2.0432 - val_accuracy: 0.5853 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 36/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.5349 - accuracy: 0.6760WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.5350 - accuracy: 0.6759 - val_loss: 2.1644 - val_accuracy: 0.5559 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 37/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.5217 - accuracy: 0.6812WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.5217 - accuracy: 0.6813 - val_loss: 2.0058 - val_accuracy: 0.5852 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.5147 - accuracy: 0.6804WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.5147 - accuracy: 0.6804 - val_loss: 2.1655 - val_accuracy: 0.5630 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 39/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.5117 - accuracy: 0.6812WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.5117 - accuracy: 0.6812 - val_loss: 1.9711 - val_accuracy: 0.5942 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 40/50\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.4998 - accuracy: 0.6859WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.4997 - accuracy: 0.6859 - val_loss: 2.2476 - val_accuracy: 0.5507 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 41/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.4906 - accuracy: 0.6893WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.4908 - accuracy: 0.6893 - val_loss: 2.0927 - val_accuracy: 0.5677 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 42/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.4826 - accuracy: 0.6906WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.4826 - accuracy: 0.6906 - val_loss: 2.1329 - val_accuracy: 0.5788 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.4864 - accuracy: 0.6910WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 1.4864 - accuracy: 0.6910 - val_loss: 1.9187 - val_accuracy: 0.6073 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 44/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.4767 - accuracy: 0.6925WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.4768 - accuracy: 0.6924 - val_loss: 1.9850 - val_accuracy: 0.5960 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 45/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.4685 - accuracy: 0.6962WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.4689 - accuracy: 0.6961 - val_loss: 2.0558 - val_accuracy: 0.5837 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 46/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.4630 - accuracy: 0.6987WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.4632 - accuracy: 0.6987 - val_loss: 2.1824 - val_accuracy: 0.5727 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 47/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.4520 - accuracy: 0.7026WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 26ms/step - loss: 1.4519 - accuracy: 0.7026 - val_loss: 2.0280 - val_accuracy: 0.5835 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 48/50\n",
            "1560/1563 [============================>.] - ETA: 0s - loss: 1.4440 - accuracy: 0.7028WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.4447 - accuracy: 0.7026 - val_loss: 2.1104 - val_accuracy: 0.5847 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 49/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.4350 - accuracy: 0.7078WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 1.4353 - accuracy: 0.7077 - val_loss: 2.0196 - val_accuracy: 0.5897 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 50/50\n",
            "1561/1563 [============================>.] - ETA: 0s - loss: 1.4344 - accuracy: 0.7077WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.4344 - accuracy: 0.7077 - val_loss: 2.0769 - val_accuracy: 0.5923 - lr: 0.0010\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.0769 - accuracy: 0.5923\n",
            "Test loss: 2.0768582820892334\n",
            "Test accuracy: 0.5922999978065491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEKw3x28tmgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c4dd52-20ae-4a8a-d054-a781f7772336"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0768582820892334, 0.5922999978065491]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2hQOj6luyGu",
        "outputId": "fa959e29-a5b4-43d7-da98-e52db9129466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "6ZIN1QP8KNx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\",\"val\"],loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "iY_CipahrwFi",
        "outputId": "c1837993-43f1-4e2c-e2e7-afaf14b9a9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ac8326e7ddbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"model Accuracy\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend([\"train\",\"val\"],loc=\"upper right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "fb7iFdbVr6oh",
        "outputId": "a9b4b9bd-6797-4923-c9ae-c91a10f734bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iOu52DAkAtSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}